{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15288 unique tokens.\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   13   34   26 3397 1808 7850\n",
      "   360    1    2 1394]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0   34   26 3397 1808 7850\n",
      "   360    1    2 1394]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0   26 3397 1808 7850\n",
      "   360    1    2 1394]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    26 3397 1808 7850]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    26 3397 1808 7850]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Dense, Activation, Dropout ,Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 60\n",
    "\n",
    "#read data\n",
    "D = pd.read_csv('../../data/text_data/train.tsv', sep='\\t', header=0)\n",
    "\n",
    "lines = D['Phrase']\n",
    "labels = D['Sentiment']\n",
    "\n",
    "#tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#split data into train and val\n",
    "n = len(lines)\n",
    "X_val = data[:int(n*0.2)]\n",
    "Y_val = labels[:int(n*0.2)]\n",
    "\n",
    "X_train = data[int(n*0.2):]\n",
    "Y_train = labels[int(n*0.2):]\n",
    "\n",
    "Y_train = to_categorical(Y_train, 5)\n",
    "Y_val = to_categorical(Y_val, 5)\n",
    "\n",
    "print(X_train[:5])\n",
    "\n",
    "if not os.path.exists('weights'):\n",
    "    os.makedirs('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijit23/miniconda3/envs/cling/lib/python3.6/site-packages/keras/engine/topology.py:1271: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    }
   ],
   "source": [
    "with open('dictionary.json') as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "model = load_model('word2vec')\n",
    "embeddings = model.get_weights()[0]\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 128))\n",
    "for word, i in word_index.items():\n",
    "    idx = dictionary.get(word, None)\n",
    "    if idx is not None:\n",
    "        embedding_matrix[i] = embeddings[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bijit23/miniconda3/envs/cling/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 60, 32)            489248    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 60, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 30, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 30, 32)            2080      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 2405      \n",
      "=================================================================\n",
      "Total params: 496,837\n",
      "Trainable params: 496,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/60\n",
      "124848/124848 [==============================] - 81s 650us/step - loss: 0.9949 - acc: 0.5956 - val_loss: 0.9278 - val_acc: 0.6285\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.92783, saving model to weights/weights_25.hdf5\n",
      "Epoch 2/60\n",
      "124848/124848 [==============================] - 80s 640us/step - loss: 0.8353 - acc: 0.6566 - val_loss: 0.9321 - val_acc: 0.6276\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.92783\n",
      "Epoch 3/60\n",
      "124848/124848 [==============================] - 80s 641us/step - loss: 0.7735 - acc: 0.6780 - val_loss: 0.9221 - val_acc: 0.6294\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.92783 to 0.92205, saving model to weights/weights_25.hdf5\n",
      "Epoch 4/60\n",
      "124848/124848 [==============================] - 80s 641us/step - loss: 0.7415 - acc: 0.6938 - val_loss: 0.9349 - val_acc: 0.6366\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0012800000607967378.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.92205\n",
      "Epoch 5/60\n",
      "124848/124848 [==============================] - 93s 742us/step - loss: 0.7055 - acc: 0.7065 - val_loss: 0.9322 - val_acc: 0.6351\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0010240000672638416.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.92205\n",
      "Epoch 6/60\n",
      "124848/124848 [==============================] - 82s 658us/step - loss: 0.6769 - acc: 0.7192 - val_loss: 0.9528 - val_acc: 0.6279\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0008192000910639763.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.92205\n",
      "Epoch 7/60\n",
      "124848/124848 [==============================] - 80s 641us/step - loss: 0.6541 - acc: 0.7270 - val_loss: 0.9584 - val_acc: 0.6258\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0006553600542247295.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.92205\n",
      "Epoch 8/60\n",
      "124848/124848 [==============================] - 81s 648us/step - loss: 0.6349 - acc: 0.7367 - val_loss: 0.9618 - val_acc: 0.6282\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005242880433797836.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.92205\n",
      "Epoch 9/60\n",
      "124848/124848 [==============================] - 80s 639us/step - loss: 0.6176 - acc: 0.7437 - val_loss: 0.9756 - val_acc: 0.6207\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0004194304347038269.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.92205\n",
      "Epoch 00009: early stopping\n",
      "Accuracy: 62.07%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 32, input_length=MAX_SEQUENCE_LENGTH)) \n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 32 #32\n",
    "adam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights/weights_25.hdf5\", verbose=1, save_best_only=True, monitor=\"val_loss\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=0, verbose=1, mode='auto', cooldown=0, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1)\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "          validation_data=(X_val, Y_val), callbacks=[reduce_lr, checkpointer, early_stopping])\n",
    "\n",
    "scores = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
