{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijit23/miniconda3/envs/cling/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15288 unique tokens.\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   13   34   26 3397 1808 7850\n",
      "   360    1    2 1394]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0   34   26 3397 1808 7850\n",
      "   360    1    2 1394]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0   26 3397 1808 7850\n",
      "   360    1    2 1394]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    26 3397 1808 7850]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    26 3397 1808 7850]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Dense, Activation, Dropout ,Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 60\n",
    "\n",
    "#read data\n",
    "D = pd.read_csv('../../data/text_data/train.tsv', sep='\\t', header=0)\n",
    "\n",
    "lines = D['Phrase']\n",
    "labels = D['Sentiment']\n",
    "\n",
    "#tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#split data into train and val\n",
    "n = len(lines)\n",
    "X_val = data[:int(n*0.2)]\n",
    "Y_val = labels[:int(n*0.2)]\n",
    "\n",
    "X_train = data[int(n*0.2):]\n",
    "Y_train = labels[int(n*0.2):]\n",
    "\n",
    "Y_train = to_categorical(Y_train, 5)\n",
    "Y_val = to_categorical(Y_val, 5)\n",
    "\n",
    "print(X_train[:5])\n",
    "\n",
    "if not os.path.exists('weights'):\n",
    "    os.makedirs('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijit23/miniconda3/envs/cling/lib/python3.6/site-packages/keras/engine/topology.py:1271: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    }
   ],
   "source": [
    "with open('dictionary.json') as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "model = load_model('word2vec')\n",
    "embeddings = model.get_weights()[0]\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 128))\n",
    "for word, i in word_index.items():\n",
    "    idx = dictionary.get(word, None)\n",
    "    if idx is not None:\n",
    "        embedding_matrix[i] = embeddings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bijit23/miniconda3/envs/cling/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 60, 32)            489248    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 508,653\n",
      "Trainable params: 508,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/60\n",
      "124848/124848 [==============================] - 240s 2ms/step - loss: 0.9835 - acc: 0.6022 - val_loss: 0.9037 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90375, saving model to weights/weights_25.hdf5\n",
      "Epoch 2/60\n",
      "124848/124848 [==============================] - 242s 2ms/step - loss: 0.7999 - acc: 0.6687 - val_loss: 0.9012 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.90375 to 0.90121, saving model to weights/weights_25.hdf5\n",
      "Epoch 3/60\n",
      "124848/124848 [==============================] - 251s 2ms/step - loss: 0.7359 - acc: 0.6922 - val_loss: 0.9060 - val_acc: 0.6478\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.90121\n",
      "Epoch 4/60\n",
      "124848/124848 [==============================] - 265s 2ms/step - loss: 0.6785 - acc: 0.7154 - val_loss: 0.9494 - val_acc: 0.6296\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0012800000607967378.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.90121\n",
      "Epoch 5/60\n",
      "124848/124848 [==============================] - 226s 2ms/step - loss: 0.6332 - acc: 0.7347 - val_loss: 0.9874 - val_acc: 0.6351\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0010240000672638416.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.90121\n",
      "Epoch 6/60\n",
      "124848/124848 [==============================] - 260s 2ms/step - loss: 0.5977 - acc: 0.7483 - val_loss: 1.0151 - val_acc: 0.6299\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0008192000910639763.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.90121\n",
      "Epoch 7/60\n",
      "124848/124848 [==============================] - 264s 2ms/step - loss: 0.5687 - acc: 0.7584 - val_loss: 1.0569 - val_acc: 0.6214\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0006553600542247295.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.90121\n",
      "Epoch 8/60\n",
      "124848/124848 [==============================] - 269s 2ms/step - loss: 0.5457 - acc: 0.7690 - val_loss: 1.0936 - val_acc: 0.6248\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005242880433797836.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.90121\n",
      "Epoch 00008: early stopping\n",
      "Accuracy: 62.48%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 32, input_length=MAX_SEQUENCE_LENGTH)) \n",
    "model.add(LSTM(50, input_shape=(1,128,)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(50, activation='relu',W_constraint=maxnorm(1)))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 32\n",
    "adam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights/weights_25.hdf5\", verbose=1, save_best_only=True, monitor=\"val_loss\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=0, verbose=1, mode='auto', cooldown=0, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1)\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "          validation_data=(X_val, Y_val), callbacks=[reduce_lr, checkpointer, early_stopping])\n",
    "\n",
    "scores = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
